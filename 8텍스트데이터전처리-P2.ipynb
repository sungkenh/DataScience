{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 텍스트 데이터 처리\n",
    "이번 예제에서는 스탠포드 대학의 [자연어처리랩](http://nlp.stanford.edu/)에서 제공하는 [교재](http://nlp.stanford.edu/IR-book/)를 사용하여 \n",
    "[텍스트처리](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)를 소개하겠다. 여기서는 단어의 어근을 찾고 단어의 기본형을 처리하는 방법을 소개한다.\n",
    "\n",
    "---\n",
    "###### 어근과 기본형\n",
    "\n",
    "언어에서는 여러가지 표현을 하기 위해서 기본 단어의 다양한 변형된 형태를 사용한다. 예를 들어 organize, organizes, organizing 등이 사용된다. 또한 비슷한 의미를 나타내기 위해서 유사한 단어들이 사용된다. 텍스트를 처리하기 위해서는 이렇게 서로 관련이 있는 단어들을 찾아내거나 변형하는 작업이 자주 필요하다. \n",
    "\n",
    "단어의 어근을 찾거나 기본형을 찾는 것은 굴절어의 표현을 종류를 줄이거나 단어의 변형된 형식의 숫자를 줄여서 기본형을 만들어 데이터 분석을 용이하게 하기 위해서 필요하다. 예를 들어 다음과 같은 작업이 필요한 경우가 많다.\n",
    "\n",
    "am, are, is ---> be  \n",
    "car, cars, car's, cars' ---> car\n",
    "\n",
    "이와같은 단어의 변형을 처리하고 나면 예를 들어 다음과 같이 문장 표현이 달라지게 된다.\n",
    "\n",
    "the boy's cars are different colors ---> the boy car be differ color\n",
    "\n",
    "하지만 모든 단어는 명확한 의미의 차이가 있다. 단어의 어근을 찾는 것은 분석을 명확하게 하기 위해서이나 동시에 정확한 단어의 사용을 무시하게 되는 결과를 얻을 수도 있다. 단어의 기본형을 찾는 작업도 단어의 원래의 의미를 놓칠 가능성이 있다.\n",
    "\n",
    "---\n",
    "\n",
    "노트: 일반적으로 기본형을 찾는 작업은 단어의 어근을 찾는 것보다 먼저 수행된다\n",
    "\n",
    "어근찾기와 기본형 찾기는 컴퓨터가 단어의 의미를 이해하는 것을 돕기 위해서 필요한 절차이다. 그러나 사람을 위해서는 원래의 표현을 그대로 살려두는 것이 필요할 수도 있다. \n",
    "\n",
    "파이선에서 제공하는 [자연처처리툴](http://www.nltk.org/)과 워드넷의 [기본형처리기](http://www.nltk.org/_modules/nltk/stem/wordnet.html)를 사용하여 단어를 변형하는 것을 설명하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import collections\n",
    "\n",
    "class WordNetLemmatizer(nltk.stem.WordNetLemmatizer):\n",
    "\t\t\"\"\"WordNetLemmatizer object. A wrapper around WordNet lemmatizer with a reverse lookup table.\"\"\"\n",
    "\t\tdef __init__(self, *args, **kwargs):\n",
    "\t\t\tsuper(self.__class__, self).__init__(*args, **kwargs)\n",
    "\t\t\tself._lem_memory = collections.defaultdict(set)\n",
    "\t\t\t# switch stem and memstem\n",
    "\t\t\tself._lem = self.lemmatize\n",
    "\t\t\tself.lem = self.memlem\n",
    "\t        \n",
    "\t\tdef memlem(self, word):\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tWrapper around WordNetLemmatizer.lemmatize that remembers the original word.\n",
    "\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tword (string): word to lemmatize\n",
    "\n",
    "\t\t\tReturns:\n",
    "\t\t\t\tlemmatized word (string)\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tlemmed_word = self._lem(word)\n",
    "\t\t\tself._lem_memory[lemmed_word].add(word)\n",
    "\t\t\treturn lemmed_word\n",
    "\t\t    \n",
    "\t\tdef unlem(self, lemmed_word):\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tUnlemmatizes the lemmatized word.\n",
    "\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tlemmed_word (string): word to unstem  \n",
    "\n",
    "\t\t\tReturns:\n",
    "\t\t\t\tlist of original words that were lemmatized to make lemmed word (list of strings). Remembering the exact original word requires storing more metadata.\n",
    "\t\t\t\"\"\"\n",
    "\t\t\treturn sorted(self._lem_memory[lemmed_word], key=len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이선이 제공하는 [자연어처리툴](http://www.nltk.org/)과 함께 어근분석기인 [SnowCastle Stemmer](http://www.nltk.org/howto/stem.html)을 사용하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SnowCastleStemmer(nltk.stem.SnowballStemmer):\n",
    "\t\t\"\"\"SnowCastleStemmer object. A wrapper around snowball stemmer with a reverse lookup table. Taken from https://gist.github.com/wassname/7fd4c975883074a99864\"\"\"\n",
    "\t\tdef __init__(self, *args, **kwargs):\n",
    "\t\t    super(self.__class__, self).__init__(*args, **kwargs)\n",
    "\t\t    self._stem_memory = collections.defaultdict(set)\n",
    "\t\t    # switch stem and memstem\n",
    "\t\t    self._stem = self.stem\n",
    "\t\t    self.stem = self.memstem\n",
    "\t\t    \n",
    "\t\tdef memstem(self, word):\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tWrapper around SnowCastleStemmer.stem that remembers the original word.\n",
    "\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tword (string): word to stem\n",
    "\n",
    "\t\t\tReturns:\n",
    "\t\t\t\tstemmed word (string)\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tstemmed_word = self._stem(word)\n",
    "\t\t\tself._stem_memory[stemmed_word].add(word)\n",
    "\t\t\treturn stemmed_word\n",
    "\t\t    \n",
    "\t\tdef unstem(self, stemmed_word):\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tUnstems the stemmed word.\n",
    "\n",
    "\t\t\tArgs:\n",
    "\t\t\t\tstemmed_word (string): word to unstem  \n",
    "\n",
    "\t\t\tReturns:\n",
    "\t\t\t\tlist of original words that were stemmed to make stemmed word (list of strings). Remembering the exact original word requires storing more metadata.\n",
    "\t\t\t\"\"\"\n",
    "\t\t\treturn sorted(self._stem_memory[stemmed_word], key=len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 기본형 찾기와 어근분석을 하여 단어를 어떻게 처리하는지를 살펴보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car -> car -> car\n",
      "cars -> car -> car\n",
      "car's -> car's -> car\n",
      "cars' -> cars' -> car\n",
      "The -> The -> the\n",
      "quick -> quick -> quick\n",
      "brown -> brown -> brown\n",
      "fox -> fox -> fox\n",
      "jumped -> jumped -> jump\n",
      "over -> over -> over\n",
      "the -> the -> the\n",
      "lazy -> lazy -> lazi\n",
      "dog -> dog -> dog\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowCastleStemmer(\"english\")\n",
    "\n",
    "def lem_and_stem(word):\n",
    "    lemmatized_word = lemmatizer.memlem(word)\n",
    "    lemmatized_and_stemmed_word = stemmer.memstem(word)\n",
    "    print(\"{} -> {} -> {}\".format(word, lemmatized_word, lemmatized_and_stemmed_word))\n",
    "    return lemmatized_and_stemmed_word\n",
    "\n",
    "words = [\"car\", \"cars\", \"car's\", \"cars'\", \"The\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "for word in words:\n",
    "    lem_and_stem(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 경우는 단순한 단어들로서 크게 바뀌지 않는 경우이다. 좀 더 복잡한 문장의 예를 살펴보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -> the -> the\n",
      "boy's -> boy's -> boy\n",
      "cars -> car -> car\n",
      "are -> are -> are\n",
      "different -> different -> differ\n",
      "colors -> color -> color\n",
      "\n",
      "the boy's cars are different colors -> the boy car are differ color\n",
      "\n",
      "The -> The -> the\n",
      "goal -> goal -> goal\n",
      "of -> of -> of\n",
      "both -> both -> both\n",
      "stemming -> stemming -> stem\n",
      "and -> and -> and\n",
      "lemmatization -> lemmatization -> lemmat\n",
      "is -> is -> is\n",
      "to -> to -> to\n",
      "reduce -> reduce -> reduc\n",
      "inflectional -> inflectional -> inflect\n",
      "forms -> form -> form\n",
      "and -> and -> and\n",
      "sometimes -> sometimes -> sometim\n",
      "derivationally -> derivationally -> deriv\n",
      "related -> related -> relat\n",
      "forms -> form -> form\n",
      "of -> of -> of\n",
      "a -> a -> a\n",
      "word -> word -> word\n",
      "to -> to -> to\n",
      "a -> a -> a\n",
      "common -> common -> common\n",
      "base -> base -> base\n",
      "form -> form -> form\n",
      "\n",
      "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form -> the goal of both stem and lemmat is to reduc inflect form and sometim deriv relat form of a word to a common base form\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"the boy's cars are different colors\", \n",
    "             \"The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form\"]\n",
    "for sentence in sentences:\n",
    "    words = sentence.split(\" \")\n",
    "    reduced_words = [lem_and_stem(word) for word in words]\n",
    "    reduced_sentence = \" \".join(reduced_words)\n",
    "    print(\"\\n{} -> {}\\n\".format(sentence, reduced_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 결과를 보면 단어가 줄어든 것을 볼 수 있다.\n",
    "\n",
    "위와 같은 과정을 거쳐서 축약된 문장으로부터 다시 원래의 문장을 복원하는 것은 어떻게 가능할까? 위의 예에서는 간단한 문장을 사용하였기 때문데 원래의 문장을 추정하는 것이 어느정도 쉽게 이루어질 수 있으나, 문장이 복잡한 경우는 원래의 문장을 제대로 복원하는 것이 어려운 작업이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -> the -> the\n",
      "boy's -> boy's -> boy\n",
      "cars -> car -> car\n",
      "are -> are -> are\n",
      "different -> different -> differ\n",
      "colors -> color -> color\n",
      "\n",
      "the boy's cars are different colors -> the boy car are differ color\n",
      "\n",
      "['the', 'The']\n",
      "the <--- ['the', 'The']\n",
      "[\"boy's\"]\n",
      "boy <--- [\"boy's\"]\n",
      "['car', 'cars', \"car's\", \"cars'\"]\n",
      "car <--- ['car', 'cars', \"car's\", \"cars'\"]\n",
      "['are']\n",
      "are <--- ['are']\n",
      "['different']\n",
      "differ <--- ['different']\n",
      "['colors']\n",
      "color <--- ['colors']\n",
      "\n",
      "the boy car are differ color -> The boy's cars' are different colors\n",
      "\n",
      "The -> The -> the\n",
      "goal -> goal -> goal\n",
      "of -> of -> of\n",
      "both -> both -> both\n",
      "stemming -> stemming -> stem\n",
      "and -> and -> and\n",
      "lemmatization -> lemmatization -> lemmat\n",
      "is -> is -> is\n",
      "to -> to -> to\n",
      "reduce -> reduce -> reduc\n",
      "inflectional -> inflectional -> inflect\n",
      "forms -> form -> form\n",
      "and -> and -> and\n",
      "sometimes -> sometimes -> sometim\n",
      "derivationally -> derivationally -> deriv\n",
      "related -> related -> relat\n",
      "forms -> form -> form\n",
      "of -> of -> of\n",
      "a -> a -> a\n",
      "word -> word -> word\n",
      "to -> to -> to\n",
      "a -> a -> a\n",
      "common -> common -> common\n",
      "base -> base -> base\n",
      "form -> form -> form\n",
      "\n",
      "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form -> the goal of both stem and lemmat is to reduc inflect form and sometim deriv relat form of a word to a common base form\n",
      "\n",
      "['the', 'The']\n",
      "the <--- ['the', 'The']\n",
      "['goal']\n",
      "goal <--- ['goal']\n",
      "['of']\n",
      "of <--- ['of']\n",
      "['both']\n",
      "both <--- ['both']\n",
      "['stemming']\n",
      "stem <--- ['stemming']\n",
      "['and']\n",
      "and <--- ['and']\n",
      "['lemmatization']\n",
      "lemmat <--- ['lemmatization']\n",
      "['is']\n",
      "is <--- ['is']\n",
      "['to']\n",
      "to <--- ['to']\n",
      "['reduce']\n",
      "reduc <--- ['reduce']\n",
      "['inflectional']\n",
      "inflect <--- ['inflectional']\n",
      "['form', 'forms']\n",
      "form <--- ['form', 'forms']\n",
      "['and']\n",
      "and <--- ['and']\n",
      "['sometimes']\n",
      "sometim <--- ['sometimes']\n",
      "['derivationally']\n",
      "deriv <--- ['derivationally']\n",
      "['related']\n",
      "relat <--- ['related']\n",
      "['form', 'forms']\n",
      "form <--- ['form', 'forms']\n",
      "['of']\n",
      "of <--- ['of']\n",
      "['a']\n",
      "a <--- ['a']\n",
      "['word']\n",
      "word <--- ['word']\n",
      "['to']\n",
      "to <--- ['to']\n",
      "['a']\n",
      "a <--- ['a']\n",
      "['common']\n",
      "common <--- ['common']\n",
      "['base']\n",
      "base <--- ['base']\n",
      "['form', 'forms']\n",
      "form <--- ['form', 'forms']\n",
      "\n",
      "the goal of both stem and lemmat is to reduc inflect form and sometim deriv relat form of a word to a common base form -> The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base forms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def unlemstem(reduced_word, lemmatizer, stemmer):\n",
    "        \"\"\"\n",
    "        Undoes the lemmatization and stemming process. \n",
    "\n",
    "        Args:\n",
    "            token (string): lemmatized and stemmed token we want to convert to its original form(s)\n",
    "\n",
    "        Returns:\n",
    "            List of original form(s) of the token. (list of strings)\n",
    "        \"\"\"\n",
    "        unstemmed = stemmer.unstem(reduced_word)\n",
    "        print(unstemmed)\n",
    "        originals = []\n",
    "        for unstem in unstemmed:\n",
    "            originals += lemmatizer.unlem(unstem)\n",
    "            \n",
    "        if len(originals) == 0 and len(unstemmed) != 0:\n",
    "            # sometimes the lemmatizer does not remember words that were only stemmed\n",
    "            originals = unstemmed\n",
    "        \n",
    "        print(\"{} <--- {}\".format(reduced_word, originals))\n",
    "        return originals\n",
    "    \n",
    "for sentence in sentences:\n",
    "    words = sentence.split(\" \")\n",
    "    reduced_words = [lem_and_stem(word) for word in words]\n",
    "    reduced_sentence = \" \".join(reduced_words)\n",
    "    print(\"\\n{} -> {}\\n\".format(sentence, reduced_sentence))\n",
    "            \n",
    "    original_words = []\n",
    "    for reduced_word in reduced_words:\n",
    "        originals = unlemstem(reduced_word, lemmatizer, stemmer)\n",
    "        sorted_originals = sorted(originals, key=len)\n",
    "        longest_original = sorted_originals[-1] # if there are multiple words, choose the longest original word\n",
    "        original_words.append(longest_original)\n",
    "    original_sentence = \" \".join(original_words)\n",
    "    print(\"\\n{} -> {}\\n\".format(reduced_sentence, original_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 방법으로, 어근찾기를 먼저 수행하고, 다음에 기본형 찾기를 수행할 수도 있다. 또한 여러가지 변형된 방법으로 텍스트를 분석해볼 수 있다. 텍스트 분석은 수치를 다루는 것이 아니므로, 다양한 문장에 대하여 기본적인 알고리즘을 적용하였을 때 실제로 어떻게 단어가 변형되는지를 확인함으로써 분석 경험을 쌓아야 한다."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
