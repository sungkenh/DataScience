{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 태양 흑점 데이터\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=files/sunspots.jpeg title=\"A white light image taken with the HMI instrument on the Solar Dynamics Observatory on 23-Oct-14 15:59:45. This is the largest sunspot group of solar cycle 24.\" style=\"width: 60%\"></p>\n",
    "\n",
    "<p style=\"text-align:center;\">*지난 25년간 가장 큰 태양 흑점 그룹*</p>\n",
    "\n",
    "[태양활동](http://www.thesuntoday.org/the-sun/solar-activity/) 은 다음과 같은 다양한 활동으로 나타나고 있다:[태양폭풍](https://en.wikipedia.org/wiki/Solar_wind), [태양화염](https://en.wikipedia.org/wiki/Solar_flare), [코로나 대량 방출](https://en.wikipedia.org/wiki/Coronal_mass_ejection), [태양 에너지 입자](https://en.wikipedia.org/wiki/Solar_energetic_particles), [흑점](https://en.wikipedia.org/wiki/Sunspot) 등이며 이들은 오랫동안 지구상에 많은 영향을 미친 것으로 알려져왔다. 예를 들어 [태양폭풍](https://en.wikipedia.org/wiki/Solar_wind)은 태양의 상류 기류에서 나오는 프라즈마의 스트림이며 이것이 지구 남극과 북극의 오로라를 만드는 원인이다.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Aurora_Borealis_and_Australis_Poster.jpg/1280px-Aurora_Borealis_and_Australis_Poster.jpg title=\"Northern and Southern Lights. Taken from wikimedia.\" style=\"width: 40%\"></p>\n",
    "\n",
    "왜 이러한 현상이 발생할까? 그리고 각각의 활동들은 서로 어떤 관련이 있을까? 그리고 이러한 활동이 지구상의 생명에 어떤 영향을 주었는지가 오랫동안 연구되어 왔다. 이 예제에서는 이러한 활동 중에 태양의 흑점에 대한 데이터를 분석하겠다.\n",
    "\n",
    "###### 흑점\n",
    "[흑점](https://en.wikipedia.org/wiki/Sunspot) 은 태양 표면에서 온도가 주위보다 낮은 영역을 가리킨다. 이러한 현상은 며칠 또는 몇달간 계속될 수 있으며, 흑점의 크기는 반경이 16km에서 16만km까지 다양하다. 흑점의 클 때에는 지구에서 눈으로 직접 확인할 수도 있다.\n",
    "\n",
    "일반적으로 흑점은 강한 자장의 활동을 나타낸다. 주변의 다른 활동과 연관되기도 하는데 다음과 같은 현상이 관련이 있는 것으로 알려져 있다: [coronal loops](https://en.wikipedia.org/wiki/Coronal_loop), [reconnection events](https://en.wikipedia.org/wiki/Magnetic_reconnection#The_Solar_Atmosphere), [solar flares](https://en.wikipedia.org/wiki/Solar_flare), 그리고 [coronal mass ejections](https://en.wikipedia.org/wiki/Coronal_mass_ejection). 오랫동안 과학자들은 흑점의 수를 파악하는 것이 태양의 활동을 예측하는데 중요한 요소로 활용하였으며 1847년부터 이러한 측정을 하였다고 한다. \n",
    "\n",
    "###### 흑점의 측정\n",
    "가장 기본적으로 흑점을 측정하는 방법은 망원경을 보면서 눈으로 흑점의 수를 세는 것이다. 최초로 이러한 측정을 한 사람은 [Rudolf Wolf](https://en.wikipedia.org/wiki/Rudolf_Wolf)인데, 사람마다 흑점을 측정한 숫자가 다르다는 문제가 있었다 (사람마다 시력이 다르므로). 루돌프 울프는 흑점이 한 지역에 모여서 나타나며 각 지역은 약 10개의 흑점이 모인다는 것을 발견하였다. 이렇게 그룹으로 나타나는 흑점과 개별적으로 나타나는 흑점에 가중치를 두어 다음과 같이 표현하는 방법을 제안하였다. \n",
    "<p style=\"text-align:center;\">`R = k(S + 10G)`</p>\n",
    "\n",
    "여기서 R은 Wolf 넘버인데, 하루에 나타나는 전체 흑점의 숫자이다. S는 개별적으로 나타나는 흑점의 갯수이고, G는 그룹으로 나타나는 흑점 그룹 수를 나타낸다. 그리고 k는 흑점을 세는 사람의 시력과 관련된 상수를 나타낸다. 예를 들어 모든 흑점을 잘 찾아내는 사람의 경우 이 값은 1이 되는 셈이다. 위에 소개한 모델은 지금까지 대체로 잘 맞는 모델로 인정받고 있다.\n",
    "\n",
    "흑점에 대해서 더 정교하게 관측하는 방법에 대해서는 다음의 논문을 참고하기 바란다.\n",
    "- https://wattsupwiththat.com/2013/01/04/counting-sunspots-and-sunspot-inflation/ \n",
    "- http://www.skyandtelescope.com/astronomy-news/how-astronomers-count-sunspots15022016513/ \n",
    "\n",
    "###### 데이터 소스\n",
    "흑점에 대한 데이터는 일반적으로 태양 데이터 센터 [Solar Influences Data Analysis Center](http://sidc.be/) (SIDC)에서 구할 수 있으며, 다음의 사이트에서 제공된다 \"[World Data Center for the production, preservation and dissemination of the international sunspot number](http://sidc.be/silso/home)\". \n",
    "\n",
    "데이터를 다음 사이트에서 얻을 수 있다. \n",
    "\n",
    "- http://sidc.be/silso/datafiles\n",
    "\n",
    "CSV 형식의 데이터를 아래에서 다운로드 받을 수 있다.\n",
    "\n",
    "- http://sidc.be/silso/INFO/sndtotcsv.php\n",
    "\n",
    "이 파일은 다음과 같은 구조로 되어있다.\n",
    "\n",
    "```\n",
    "CSV  \n",
    "-------------------------------------------------------------------------------  \n",
    "Filename: SN_d_tot_V2.0.csv\n",
    "Format: Comma Separated values (adapted for import in spreadsheets)\n",
    "The separator is the semicolon ';'.\n",
    "\n",
    "Contents:  \n",
    "Column 1-3: Gregorian calendar date\n",
    "- Year\n",
    "- Month\n",
    "- Day  \n",
    "\n",
    "Column 4: Date in fraction of year.  \n",
    "Column 5: Daily total sunspot number. A value of -1 indicates that no number is available for that day (missing value).  \n",
    "Column 6: Daily standard deviation of the input sunspot numbers from individual stations.  \n",
    "Column 7: Number of observations used to compute the daily value.  \n",
    "Column 8: Definitive/provisional indicator. '1' indicates that the value is definitive. '0' indicates that the value is still provisional.  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1 단계 : 데이터 저장소 확인\n",
    "데이터를 저장할 곳을 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 아래에서 '.'는 현재 작업 디렉토리를 가리킨다 (즉, 현재 프로그램이 저장되어 있는곳을 말한다)\n",
    "# 이렇게 현재 디렉토리를 중심으로 프로그램을 작성함으로써 프로그램에서 절대 경로를 다루지 않아도 되어 편리하다\n",
    "# 아래는 고정된 이름의 디렉토리를 정의하는데 이와 같이 상수화된 변수는 프로그램의 첫부분에서 정의하는 것이 일반적이다\n",
    "\n",
    "path_to_directory = \"./data\"\n",
    "path_to_data = \"./data/sunspots\"\n",
    "\n",
    "# 아래에서 os 는 운영체제와 통신하기 위해 사용하는 라이브러리이다\n",
    "# 이 라이브러리를 사용하면 운영 체제에게 임의의 명령문을 내릴 수 있다\n",
    "import os  \n",
    "\n",
    "if not os.path.isdir(path_to_directory): # if there is no data directory, create one\n",
    "    os.mkdir(path_to_directory)\n",
    "\n",
    "# shutil은 파일과 디렉터리를 다루는 유틸리티이다\n",
    "# 기존에 같은 이름의 디렉토리가 존재하는지 확인하고 그 내용을 비우는 작업을 한다\n",
    "# 이렇게 하는 목적은 새로운 데이터셋을 안전하게 사용하기 위해서이다\n",
    "import shutil\n",
    "    \n",
    "if os.path.isdir(path_to_data):\n",
    "    shutil.rmtree(path_to_data) # 디렉토리와 그 내용을 모두 지운다\n",
    "\n",
    "os.mkdir(path_to_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  2 단계 : 데이터 다운로드\n",
    "이제 데이터를 다운로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# requests는 URL을 다루기 위해 사용한다 \n",
    "import requests\n",
    "\n",
    "# 여기서는 목적지 URL 주소를 직접 입력했다\n",
    "# 만일 입력할 목적지 주소가 많다면 Requests나 BeautifulSoup4 라이브러리를 사용하여\n",
    "# 웹 사이트에서 자동으로 링크 주소를 얻는 방법도 있다\n",
    "\n",
    "url = \"http://sidc.be/silso/INFO/sndtotcsv.php\"\n",
    "name = \"sunspots.csv\"\n",
    "path = os.path.join(path_to_data, name)\n",
    "\n",
    "response = requests.get(url) \n",
    "if response.status_code == 200: # HTTP response code 200 은 \"OK\"를 의미한다\n",
    "    # with open() as f과 같이 사용하면, 파일의 사용이 종료되면 자동으로 파일을 닫는다\n",
    "    with open(path, 'w') as outfile: # 'w' 는 (w)rite를 의미한다.\n",
    "        outfile.write(response.text)  # HTTP response 내용을 출력 파일로 복사한다\n",
    "else:\n",
    "    # 응답이 \"OK\"가 아니면 아래의 문자을 출력하고 프로그램을 종료하게 한다.\n",
    "    raise ValueError(\"Requests returned a non-200 HTTP status code (status: {}) when downloading {}.\".format(r.status_code, url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3 단계 : 로드 데이터\n",
    "태양 흑점 데이터는 콤마로 데이터가 구분된 파일로 제공되는데, 이를 직접 프로그램에서 다루기가 쉽지 않다. 이를 다루기 쉬운 포맷으로 바꾸기 위해서 [Pandas](http://pandas.pydata.org/)의 [dataframe](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)을 사용하는데 이는 파이선에서 테이블 구조의 데이터를 쉽게 다루게 한다. 여기서는 판다가 제공하는 편리한 [read_csv](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) 기능을 사용하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pandas는 파이선에서 데이터분석을 위해 가장 기본적으로 사용되는 라이브러리이다\n",
    "import pandas as pd\n",
    "\n",
    "# header=none 은 헤더가 없다는 것을 나타낸다\n",
    "# delimiter=\";\" 각 행이 탭으로 구분된다는 것을 나타낸다\n",
    "# skipinitialspace 은 여백(스페이스)를 무시하여 CSV 파일을 눈으로 보기 쉽게 만들어 준다\n",
    "data = pd.read_csv(path, header=None, delimiter=\";\", skipinitialspace=True)\n",
    "\n",
    "# 헤더가 없으므로 직접 레이블을 명명해주어야 한다.\n",
    "data.columns = ['year', 'month', 'day', 'date_as_fraction_of_year', 'combined_count', 'std', 'num_observations', 'definitive_or_not']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4 단계 : 데이터 체크\n",
    "데이터가 원하는 데이터인지를 확인하기 위해 간단한 확인을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 8개의 컬럼이 있는지 확인한다\n",
    "columns = data.columns\n",
    "assert(len(columns) == 8)\n",
    "\n",
    "# 각 컬럼이 제대로 되어 있는지 확인한다\n",
    "expected_columns = ['year', 'month', 'day', 'date_as_fraction_of_year', 'combined_count', 'std', 'num_observations', 'definitive_or_not']\n",
    "# 두개의 리스트의 값이 일치하는지를 == 를 사용하여 확인한다\n",
    "# .all()을 사용하면 각 페어의 값이 서로 일치하는지 확인할 수 있다\n",
    "assert((columns == expected_columns).all())\n",
    "\n",
    "# 1818년 부터 2016년까지 데이터가 있는지 확인한다\n",
    "years = set(data['year'].values.tolist())\n",
    "expected_values = set(range(1818, 2017))\n",
    "remaining = [year for year in years if year not in expected_values]\n",
    "assert((years == expected_values))    \n",
    "\n",
    "# 매년 1월부터 12월까지 적어도 한번의 데이터가 포함되어 있는지 확인한다\n",
    "months = set(data['month'].values.tolist())\n",
    "expected_values = set(range(1, 13))\n",
    "assert((months == expected_values))\n",
    "\n",
    "# 매월 1일부터 31일까지 각각 데이터가 들어 있는지 확인한다\n",
    "days = set(data['day'].values.tolist())\n",
    "expected_values = set(range(1, 32))\n",
    "assert((days == expected_values))\n",
    "\n",
    "# 위에서 소개한 것과 같이 매일의 데이터가 들어있는지 확인하는 방법도 있겠지만\n",
    "# 전 기간의 날짜를 계산하여 데이터가 필요한 갯수만큼 들어있는지를 확인하는 방법도 있다.\n",
    "# 어떤 방법을 사용하든 데이터 분석 결과에서 오류가 나지 않도록 하는 방법이 가장 안전하다.\n",
    "\n",
    "num_examples = len(data.index.values.tolist())\n",
    "num_years = num_examples / 365.25\n",
    "expected_num_years = 2016-1818\n",
    "\n",
    "# 2016년 데이터도 확인한다\n",
    "assert(expected_num_years <= num_years <= (expected_num_years + 1))\n",
    "\n",
    "definitive_or_not = set(data['definitive_or_not'].values.tolist())\n",
    "expected_values = set([0, 1])\n",
    "assert((definitive_or_not == expected_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터를 다운로드하고,저장하고, 분석을 하기 위한 기본적인 내용을 검증하였다.\n",
    "\n",
    "###### 5 단계 : 코드 재구성\n",
    "\n",
    "지금까지 작성된 코드를 하나로 재구성하였다. load_news.py 파일로 만들고 메인 함수는 아래와 같이 호출하면 된다.\n",
    "\n",
    "`data = load_sunspot_counts()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "path_to_directory = \"./data\"\n",
    "path_to_data = os.path.join(path_to_directory, \"sunspots\")\n",
    "\n",
    "url = \"http://sidc.be/silso/INFO/sndtotcsv.php\"\n",
    "name = \"sunspots.csv\"\n",
    "path = os.path.join(path_to_data, name)\n",
    "\n",
    "def check_destination():\n",
    "    if not os.path.isdir(path_to_directory):\n",
    "        os.mkdir(path_to_directory)\n",
    "    \n",
    "    if not os.path.isdir(path_to_data):\n",
    "        os.mkdir(path_to_data)\n",
    "\n",
    "def download_data(): \n",
    "    if not os.path.exists(path):\n",
    "        response = requests.get(url) \n",
    "        if response.status_code == 200: # HTTP response code 200 은 \"OK\"를 의미한다\n",
    "            # with open() as f과 같이 사용하면, 파일의 사용이 종료되면 자동으로 파일을 닫는다\n",
    "            with open(path, 'w') as outfile: # 'w' 는 (w)rite를 의미한다.\n",
    "                outfile.write(response.text)  # HTTP response 내용을 출력 파일로 복사한다\n",
    "        else:\n",
    "            # 응답이 \"OK\"가 아니면 아래의 문자을 출력하고 프로그램을 종료하게 한다.\n",
    "            raise ValueError(\"Requests returned a non-200 HTTP status code (status: {}) when downloading {}.\".format(r.status_code, url))\n",
    "    \n",
    "def verify_data(data):\n",
    "    # 8개의 컬럼이 있는지 확인한다\n",
    "    columns = data.columns\n",
    "    assert(len(columns) == 8)\n",
    "\n",
    "    # 각 컬럼이 제대로 되어 있는지 확인한다\n",
    "    expected_columns = ['year', 'month', 'day', 'date_as_fraction_of_year', 'combined_count', 'std', 'num_observations', 'definitive_or_not']\n",
    "    assert((columns == expected_columns).all())\n",
    "\n",
    "    # 1818년 부터 2016년까지 데이터가 있는지 확인한다\n",
    "    years = set(data['year'].values.tolist())\n",
    "    expected_values = set(range(1818, 2017))\n",
    "    remaining = [year for year in years if year not in expected_values]\n",
    "    assert((years == expected_values))    \n",
    "\n",
    "    # 매년 1월부터 12월까지 적어도 한번의 데이터가 포함되어 있는지 확인한다\n",
    "    months = set(data['month'].values.tolist())\n",
    "    expected_values = set(range(1, 13))\n",
    "    assert((months == expected_values))\n",
    "\n",
    "    # 매월 1일부터 31일까지 각각 데이터가 들어 있는지 확인한다\n",
    "    days = set(data['day'].values.tolist())\n",
    "    expected_values = set(range(1, 32))\n",
    "    assert((days == expected_values))\n",
    "\n",
    "    # 전 기간의 데이터 갯수와 일치하는지를 확인한다\n",
    "    num_examples = len(data.index.values.tolist())\n",
    "    num_years = num_examples / 365.25\n",
    "    expected_num_years = 2016-1818\n",
    "    \n",
    "    # 2016년 데이터도 확인한다\n",
    "    assert(expected_num_years <= num_years <= (expected_num_years + 1))\n",
    "\n",
    "    # defininite 나 provisional 값이 부울 값인지 확인한다\n",
    "    definitive_or_not = set(data['definitive_or_not'].values.tolist())\n",
    "    expected_values = set([0, 1])\n",
    "    assert((definitive_or_not == expected_values))\n",
    "            \n",
    "def load_sunspot_counts():\n",
    "    check_destination()\n",
    "    download_data()\n",
    "    data = pd.read_csv(path, header=None, delimiter=\";\", skipinitialspace=True)\n",
    "    data.columns = ['year', 'month', 'day', 'date_as_fraction_of_year', 'combined_count', 'std', 'num_observations', 'definitive_or_not']\n",
    "    verify_data(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       year  month  day  date_as_fraction_of_year  combined_count   std  \\\n",
      "0      1818      1    1                  1818.001              -1  -1.0   \n",
      "1      1818      1    2                  1818.004              -1  -1.0   \n",
      "2      1818      1    3                  1818.007              -1  -1.0   \n",
      "3      1818      1    4                  1818.010              -1  -1.0   \n",
      "4      1818      1    5                  1818.012              -1  -1.0   \n",
      "5      1818      1    6                  1818.015              -1  -1.0   \n",
      "6      1818      1    7                  1818.018              -1  -1.0   \n",
      "7      1818      1    8                  1818.021              65  10.2   \n",
      "8      1818      1    9                  1818.023              -1  -1.0   \n",
      "9      1818      1   10                  1818.026              -1  -1.0   \n",
      "10     1818      1   11                  1818.029              -1  -1.0   \n",
      "11     1818      1   12                  1818.032              -1  -1.0   \n",
      "12     1818      1   13                  1818.034              37   7.7   \n",
      "13     1818      1   14                  1818.037              -1  -1.0   \n",
      "14     1818      1   15                  1818.040              -1  -1.0   \n",
      "15     1818      1   16                  1818.042              -1  -1.0   \n",
      "16     1818      1   17                  1818.045              77  11.1   \n",
      "17     1818      1   18                  1818.048              98  12.6   \n",
      "18     1818      1   19                  1818.051             105  13.0   \n",
      "19     1818      1   20                  1818.053              -1  -1.0   \n",
      "20     1818      1   21                  1818.056              -1  -1.0   \n",
      "21     1818      1   22                  1818.059              -1  -1.0   \n",
      "22     1818      1   23                  1818.062              -1  -1.0   \n",
      "23     1818      1   24                  1818.064              -1  -1.0   \n",
      "24     1818      1   25                  1818.067              25   6.3   \n",
      "25     1818      1   26                  1818.070              -1  -1.0   \n",
      "26     1818      1   27                  1818.073              -1  -1.0   \n",
      "27     1818      1   28                  1818.075              38   7.8   \n",
      "28     1818      1   29                  1818.078              20   5.7   \n",
      "29     1818      1   30                  1818.081              -1  -1.0   \n",
      "...     ...    ...  ...                       ...             ...   ...   \n",
      "72593  2016     10    2                  2016.753              15   5.5   \n",
      "72594  2016     10    3                  2016.755              36   3.5   \n",
      "72595  2016     10    4                  2016.758              40   3.3   \n",
      "72596  2016     10    5                  2016.761              41   2.9   \n",
      "72597  2016     10    6                  2016.764              56   3.7   \n",
      "72598  2016     10    7                  2016.766              59   3.8   \n",
      "72599  2016     10    8                  2016.769              63   5.0   \n",
      "72600  2016     10    9                  2016.772              69   6.3   \n",
      "72601  2016     10   10                  2016.775              66   4.0   \n",
      "72602  2016     10   11                  2016.777              64   4.6   \n",
      "72603  2016     10   12                  2016.780              41   3.4   \n",
      "72604  2016     10   13                  2016.783              41   4.0   \n",
      "72605  2016     10   14                  2016.786              41   3.3   \n",
      "72606  2016     10   15                  2016.788              36   1.8   \n",
      "72607  2016     10   16                  2016.791              32   2.6   \n",
      "72608  2016     10   17                  2016.794              25   1.7   \n",
      "72609  2016     10   18                  2016.796              29   1.1   \n",
      "72610  2016     10   19                  2016.799              30   2.1   \n",
      "72611  2016     10   20                  2016.802              16   1.4   \n",
      "72612  2016     10   21                  2016.805              28   2.1   \n",
      "72613  2016     10   22                  2016.807              28   1.4   \n",
      "72614  2016     10   23                  2016.810              15   1.2   \n",
      "72615  2016     10   24                  2016.813              15   1.2   \n",
      "72616  2016     10   25                  2016.816              17   0.9   \n",
      "72617  2016     10   26                  2016.818              29   1.9   \n",
      "72618  2016     10   27                  2016.821              25   1.4   \n",
      "72619  2016     10   28                  2016.824              36   1.8   \n",
      "72620  2016     10   29                  2016.827              23   2.2   \n",
      "72621  2016     10   30                  2016.829              14   1.0   \n",
      "72622  2016     10   31                  2016.832              13   1.2   \n",
      "\n",
      "       num_observations  definitive_or_not  \n",
      "0                     0                  1  \n",
      "1                     0                  1  \n",
      "2                     0                  1  \n",
      "3                     0                  1  \n",
      "4                     0                  1  \n",
      "5                     0                  1  \n",
      "6                     0                  1  \n",
      "7                     1                  1  \n",
      "8                     0                  1  \n",
      "9                     0                  1  \n",
      "10                    0                  1  \n",
      "11                    0                  1  \n",
      "12                    1                  1  \n",
      "13                    0                  1  \n",
      "14                    0                  1  \n",
      "15                    0                  1  \n",
      "16                    1                  1  \n",
      "17                    1                  1  \n",
      "18                    1                  1  \n",
      "19                    0                  1  \n",
      "20                    0                  1  \n",
      "21                    0                  1  \n",
      "22                    0                  1  \n",
      "23                    0                  1  \n",
      "24                    1                  1  \n",
      "25                    0                  1  \n",
      "26                    0                  1  \n",
      "27                    1                  1  \n",
      "28                    1                  1  \n",
      "29                    0                  1  \n",
      "...                 ...                ...  \n",
      "72593                26                  0  \n",
      "72594                28                  0  \n",
      "72595                33                  0  \n",
      "72596                28                  0  \n",
      "72597                32                  0  \n",
      "72598                22                  0  \n",
      "72599                29                  0  \n",
      "72600                28                  0  \n",
      "72601                30                  0  \n",
      "72602                26                  0  \n",
      "72603                22                  0  \n",
      "72604                30                  0  \n",
      "72605                16                  0  \n",
      "72606                14                  0  \n",
      "72607                12                  0  \n",
      "72608                 7                  0  \n",
      "72609                21                  0  \n",
      "72610                10                  0  \n",
      "72611                29                  0  \n",
      "72612                25                  0  \n",
      "72613                27                  0  \n",
      "72614                17                  0  \n",
      "72615                12                  0  \n",
      "72616                16                  0  \n",
      "72617                15                  0  \n",
      "72618                14                  0  \n",
      "72619                13                  0  \n",
      "72620                 8                  0  \n",
      "72621                34                  0  \n",
      "72622                40                  0  \n",
      "\n",
      "[72623 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "data = load_sunspot_counts()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
